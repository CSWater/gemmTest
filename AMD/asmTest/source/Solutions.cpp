/*******************************************************************************
* Copyright (C) 2016 Advanced Micro Devices, Inc. All rights reserved.
*
* Permission is hereby granted, free of charge, to any person obtaining a copy
* of this software and associated documentation files (the "Software"), to deal
* in the Software without restriction, including without limitation the rights
* to use, copy, modify, merge, publish, distribute, sublicense, and/or sell cop-
* ies of the Software, and to permit persons to whom the Software is furnished
* to do so, subject to the following conditions:
*
* The above copyright notice and this permission notice shall be included in all
* copies or substantial portions of the Software.
*
* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IM-
* PLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
* FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
* COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
* IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNE-
* CTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
*******************************************************************************/

/**************************************************
* This file was generated by Tensile:             *
* https://github.com/ROCmSoftwarePlatform/Tensile *
**************************************************/


#include "Solutions.h"
#include <algorithm>
TensileStatus Cijk_Ailk_Bjlk_DB_MT064x032x04_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_08_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT064x032x04_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_08_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 4 };
  const unsigned int groupSize[2] = { 16, 8 };
  size_t localWorkSize[3] = { 128, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT064x032x04_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_08_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 1
  BufferStore: True
  NumLoadsA: 1
  NumLoadsB: 1
  BufferLoad: True
  SubGroupA: 16
  SubGroupB: 8
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 4]
  SubGroup0: 16
  ThreadTileB: 4
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 768
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 4
  LSPB: 4
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [16, 8, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 8
  LVCB: 32
  LVCA: 32
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 4
  UnrollMemFence: False
  LSPA: 4
  GlobalLoadVectorWidthA: 2
  GlobalWriteVectorWidth: 2
  LdsNumElements: 896
  SubGroup1: 8
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 64
  PersistentKernel: 0
  LSCB: 32
  GuaranteeNoPartialA: False
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: True
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 4
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 256
  LdsNumElementsAlignedB: 128
  MinGlobalWriteVectorWidth: 1
  LVPA: 2
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 32
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 64
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 16
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 512
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 64
  MacroTile1: 32
  NumThreads: 128
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 256
  NumLoadsPerpendicularB: 1
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT032x064x04_NLCA01_NLCB01_TT04_04_USFGRO0_WG08_16_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT032x064x04_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG08_16_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 4 };
  const unsigned int groupSize[2] = { 8, 16 };
  size_t localWorkSize[3] = { 128, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT032x064x04_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG08_16_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 2
  BufferStore: True
  NumLoadsA: 1
  NumLoadsB: 1
  BufferLoad: True
  SubGroupA: 8
  SubGroupB: 16
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 4]
  SubGroup0: 8
  ThreadTileB: 4
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 640
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 2
  LSPB: 4
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [8, 16, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 8
  LVCB: 32
  LVCA: 32
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 4
  UnrollMemFence: False
  LSPA: 4
  GlobalLoadVectorWidthA: 1
  GlobalWriteVectorWidth: 2
  LdsNumElements: 896
  SubGroup1: 16
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 32
  PersistentKernel: 0
  LSCB: 64
  GuaranteeNoPartialA: True
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: False
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 4
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 128
  LdsNumElementsAlignedB: 256
  MinGlobalWriteVectorWidth: 1
  LVPA: 4
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 64
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 32
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 16
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 512
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 32
  MacroTile1: 64
  NumThreads: 128
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 128
  NumLoadsPerpendicularB: 1
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT064x064x04_NLCA01_NLCB01_TT04_04_USFGRO01_WG16_16_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT064x064x04_K1_NLCA01_NLCB01_TT04_04_USFGRO01_WG16_16_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 4 };
  const unsigned int groupSize[2] = { 16, 16 };
  size_t localWorkSize[3] = { 256, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT064x064x04_K1_NLCA01_NLCB01_TT04_04_USFGRO01_WG16_16_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 1
  BufferStore: True
  NumLoadsA: 1
  NumLoadsB: 1
  BufferLoad: True
  SubGroupA: 16
  SubGroupB: 16
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 4]
  SubGroup0: 16
  ThreadTileB: 4
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 768
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 4
  LSPB: 4
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [16, 16, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 8
  LVCB: 64
  LVCA: 64
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 4
  UnrollMemFence: False
  LSPA: 4
  GlobalLoadVectorWidthA: 1
  GlobalWriteVectorWidth: 2
  LdsNumElements: 1024
  SubGroup1: 16
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 64
  PersistentKernel: 0
  LSCB: 64
  GuaranteeNoPartialA: True
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: True
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 4
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 256
  LdsNumElementsAlignedB: 256
  MinGlobalWriteVectorWidth: 1
  LVPA: 4
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 64
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 64
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: 1
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 16
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 512
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 64
  MacroTile1: 64
  NumThreads: 256
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 256
  NumLoadsPerpendicularB: 1
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT064x032x08_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_08_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT064x032x08_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_08_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 4 };
  const unsigned int groupSize[2] = { 16, 8 };
  size_t localWorkSize[3] = { 128, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT064x032x08_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_08_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 2
  BufferStore: True
  NumLoadsA: 2
  NumLoadsB: 1
  BufferLoad: True
  SubGroupA: 16
  SubGroupB: 8
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 4]
  SubGroup0: 16
  ThreadTileB: 4
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 1536
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 4
  LSPB: 8
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [16, 8, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 8
  LVCB: 16
  LVCA: 32
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 8
  UnrollMemFence: False
  LSPA: 4
  GlobalLoadVectorWidthA: 2
  GlobalWriteVectorWidth: 2
  LdsNumElements: 1792
  SubGroup1: 8
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 64
  PersistentKernel: 0
  LSCB: 32
  GuaranteeNoPartialA: False
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: False
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 8
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 512
  LdsNumElementsAlignedB: 256
  MinGlobalWriteVectorWidth: 1
  LVPA: 2
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 32
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 64
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 16
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 1024
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 64
  MacroTile1: 32
  NumThreads: 128
  NumLoadsPerpendicularA: 2
  LdsOffsetA: 0
  LdsOffsetB: 512
  NumLoadsPerpendicularB: 1
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT096x032x08_NLCA03_NLCB01_TT06_04_USFGRO0_WG16_08_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT096x032x08_K1_NLCA03_NLCB01_TT06_04_USFGRO0_WG16_08_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 6, 4 };
  const unsigned int groupSize[2] = { 16, 8 };
  size_t localWorkSize[3] = { 128, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT096x032x08_K1_NLCA03_NLCB01_TT06_04_USFGRO0_WG16_08_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 2
  BufferStore: True
  NumLoadsA: 3
  NumLoadsB: 1
  BufferLoad: True
  SubGroupA: 16
  SubGroupB: 8
  ThreadTile0: 6
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [6, 4]
  SubGroup0: 16
  ThreadTileB: 4
  ThreadTileA: 6
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 1792
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 4
  LSPB: 8
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [16, 8, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 3
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 12
  LVCB: 16
  LVCA: 16
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 8
  UnrollMemFence: False
  LSPA: 8
  GlobalLoadVectorWidthA: 2
  GlobalWriteVectorWidth: 2
  LdsNumElements: 2048
  SubGroup1: 8
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 32
  PersistentKernel: 0
  LSCB: 32
  GuaranteeNoPartialA: False
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: False
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 8
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 768
  LdsNumElementsAlignedB: 256
  MinGlobalWriteVectorWidth: 1
  LVPA: 4
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 32
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 96
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 24
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 1024
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 96
  MacroTile1: 32
  NumThreads: 128
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 768
  NumLoadsPerpendicularB: 1
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT032x064x08_NLCA01_NLCB01_TT04_04_USFGRO0_WG08_16_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT032x064x08_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG08_16_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 4 };
  const unsigned int groupSize[2] = { 8, 16 };
  size_t localWorkSize[3] = { 128, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT032x064x08_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG08_16_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 2
  BufferStore: True
  NumLoadsA: 1
  NumLoadsB: 2
  BufferLoad: True
  SubGroupA: 8
  SubGroupB: 16
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 4]
  SubGroup0: 8
  ThreadTileB: 4
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 1280
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 2
  LSPB: 4
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [8, 16, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 8
  LVCB: 32
  LVCA: 16
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 8
  UnrollMemFence: False
  LSPA: 8
  GlobalLoadVectorWidthA: 2
  GlobalWriteVectorWidth: 2
  LdsNumElements: 1792
  SubGroup1: 16
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 32
  PersistentKernel: 0
  LSCB: 64
  GuaranteeNoPartialA: False
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: False
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 8
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 256
  LdsNumElementsAlignedB: 512
  MinGlobalWriteVectorWidth: 1
  LVPA: 4
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 64
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 32
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 16
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 1024
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 32
  MacroTile1: 64
  NumThreads: 128
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 256
  NumLoadsPerpendicularB: 2
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT032x096x08_NLCA01_NLCB03_TT04_06_USFGRO0_WG08_16_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT032x096x08_K1_NLCA01_NLCB03_TT04_06_USFGRO0_WG08_16_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 6 };
  const unsigned int groupSize[2] = { 8, 16 };
  size_t localWorkSize[3] = { 128, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT032x096x08_K1_NLCA01_NLCB03_TT04_06_USFGRO0_WG08_16_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 2
  BufferStore: True
  NumLoadsA: 1
  NumLoadsB: 3
  BufferLoad: True
  SubGroupA: 8
  SubGroupB: 16
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 6]
  SubGroup0: 8
  ThreadTileB: 6
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 1280
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 4
  LSPB: 8
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [8, 16, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 3
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 12
  LVCB: 16
  LVCA: 16
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 6
  DepthU: 8
  UnrollMemFence: False
  LSPA: 8
  GlobalLoadVectorWidthA: 2
  GlobalWriteVectorWidth: 2
  LdsNumElements: 2048
  SubGroup1: 16
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 32
  PersistentKernel: 0
  LSCB: 32
  GuaranteeNoPartialA: False
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: False
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 8
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 256
  LdsNumElementsAlignedB: 768
  MinGlobalWriteVectorWidth: 1
  LVPA: 4
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 96
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 32
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 24
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 1024
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 32
  MacroTile1: 96
  NumThreads: 128
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 256
  NumLoadsPerpendicularB: 1
*/

TensileStatus Cijk_Ailk_Bjlk_DB_MT064x064x08_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_16_01(
    double * dataC,
    const double * dataA,
    const double * dataB,
    double alpha,
    double beta,
    unsigned int offsetC,
    unsigned int offsetA,
    unsigned int offsetB,
    unsigned int strideC1J,
    unsigned int strideC2K,
    unsigned int strideA1L,
    unsigned int strideA2K,
    unsigned int strideB1L,
    unsigned int strideB2K,
    unsigned int sizeI,
    unsigned int sizeJ,
    unsigned int sizeK,
    unsigned int sizeL,
    hipStream_t stream,
    unsigned int numInputEvents,
    hipEvent_t * inputEvents,
    hipEvent_t * outputEvent) {
  TensileStatus status;

  /* module function args */
  struct {
    // Size of lowest Tensor's lowest 2 dims, in bytes.  Does not include bath dim or higher (>2) order dimensions
    uint64_t tensor2dSizeC;
    uint64_t tensor2dSizeA;
    uint64_t tensor2dSizeB;
    double * dataC;
    const double * dataA;
    const double * dataB;
    double alpha;
    double beta;
    unsigned int offsetC;
    unsigned int offsetA;
    unsigned int offsetB;
    unsigned int strideC1J;
    unsigned int strideC2K;
    unsigned int strideA1L;
    unsigned int strideA2K;
    unsigned int strideB1L;
    unsigned int strideB2K;
    unsigned int sizeI;
    unsigned int sizeJ;
    unsigned int sizeK;
    unsigned int sizeL;
    unsigned int pad;
  } hipFunctionArgs;
  size_t hipFunctionArgsSize = sizeof(hipFunctionArgs);
  void *hipLaunchParams[] = {HIP_LAUNCH_PARAM_BUFFER_POINTER, &hipFunctionArgs, HIP_LAUNCH_PARAM_BUFFER_SIZE, &hipFunctionArgsSize, HIP_LAUNCH_PARAM_END};
  int deviceId;
  hipGetDevice(&deviceId);

  /* kernels */
  const unsigned int numKernels = 1; // 1 or 4
  hipFunction_t hipFunction;
  static SolutionLock sl;
  status = sl.getFunction(&hipFunction, deviceId, "Cijk_Ailk_Bjlk_DB_MT064x064x08_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_16_01", nullptr);;
  if (status) return status;

  /* num kernels */
  unsigned int numEnqueues[numKernels] = { 1 };

  /* grid sizes */
  const unsigned int workDim = 3;
  const unsigned int threadTile[2] = { 4, 4 };
  const unsigned int groupSize[2] = { 16, 16 };
  size_t localWorkSize[3] = { 256, 1, 1 };
  size_t globalWorkSize[numKernels][3];
  globalWorkSize[0][2] = 1;
  globalWorkSize[0][2] *= sizeK;
  unsigned int sizeOfC0 = sizeI;
  unsigned int sizeOfC1 = sizeJ;
  unsigned int macroTile0 = static_cast<unsigned int>(groupSize[0] * threadTile[0]);
  unsigned int macroTile1 = static_cast<unsigned int>(groupSize[1] * threadTile[1]);
  unsigned int totalWorkGroups0 = sizeOfC0 / macroTile0;
  unsigned int totalWorkGroups1 = sizeOfC1 / macroTile1;
  // b/c single kernel, add extra work-group here if edge needed
  if (totalWorkGroups0*macroTile0 < sizeOfC0) { totalWorkGroups0++; }
  if (totalWorkGroups1*macroTile1 < sizeOfC1) { totalWorkGroups1++; }
  globalWorkSize[0][0] = totalWorkGroups0;
  globalWorkSize[0][1] = totalWorkGroups1;

  /* offsets */
  unsigned int offsets[numKernels][1][3];
  offsets[0][0][0] = offsetC; // tensorC
  offsets[0][0][1] = offsetA; // tensorA
  offsets[0][0][2] = offsetB; // tensorB

  /* index sizes */
  unsigned int sizes[numKernels][1][4];
  sizes[0][0][0] = sizeI;
  sizes[0][0][1] = sizeJ;
  sizes[0][0][2] = sizeK;
  sizes[0][0][3] = sizeL;
  uint64_t tensor2dSizeC = 1 * std::max(sizeI, strideA1L) * sizeJ;
  uint64_t tensor2dSizeA = 1 * std::max(sizeI, strideA1L) * sizeL;
  uint64_t tensor2dSizeB = 1 * std::max(sizeJ, strideB1L) * sizeL;


  /* kernel 0: Cijk_Ailk_Bjlk_DB_MT064x064x08_K1_NLCA01_NLCB01_TT04_04_USFGRO0_WG16_16_01 */
  unsigned int kernelIdx = 0;
  for (unsigned int enqueueIdx = 0; enqueueIdx < numEnqueues[0]; enqueueIdx++) {
  try {
    hipFunctionArgs.dataC = dataC;
    hipFunctionArgs.dataA = dataA;
    hipFunctionArgs.dataB = dataB;
    hipFunctionArgs.alpha = alpha;
    hipFunctionArgs.beta = beta;
    hipFunctionArgs.offsetC = offsets[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.offsetA = offsets[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.offsetB = offsets[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.strideC1J = strideC1J;
    hipFunctionArgs.strideC2K = strideC2K;
    hipFunctionArgs.strideA1L = strideA1L;
    hipFunctionArgs.strideA2K = strideA2K;
    hipFunctionArgs.strideB1L = strideB1L;
    hipFunctionArgs.strideB2K = strideB2K;
    hipFunctionArgs.sizeI = sizes[kernelIdx][enqueueIdx][0];
    hipFunctionArgs.sizeJ = sizes[kernelIdx][enqueueIdx][1];
    hipFunctionArgs.sizeK = sizes[kernelIdx][enqueueIdx][2];
    hipFunctionArgs.sizeL = sizes[kernelIdx][enqueueIdx][3];
    hipFunctionArgs.tensor2dSizeC = tensor2dSizeC;
    hipFunctionArgs.tensor2dSizeA = tensor2dSizeA;
    hipFunctionArgs.tensor2dSizeB = tensor2dSizeB;
    hipHccModuleLaunchKernel(
      hipFunction,
      globalWorkSize[kernelIdx][0]*localWorkSize[0],
      globalWorkSize[kernelIdx][1]*localWorkSize[1],
      globalWorkSize[kernelIdx][2]*localWorkSize[2],
      localWorkSize[0],
      localWorkSize[1],
      localWorkSize[2],
      0, // groupMemBytes
      stream,
      NULL,
      (void**)hipLaunchParams
      ,inputEvents ? inputEvents[enqueueIdx]:nullptr
      ,outputEvent ? outputEvent[enqueueIdx]:nullptr
      );
  } catch (const std::exception& e) {
#ifdef DEBUG
    std::cerr << e.what() << std::endl;
#endif
    return tensileStatusFailure;
  }
  }

  return tensileStatusSuccess;
}

/* Solution Parameters
  ProblemType: Cijk_Ailk_Bjlk_DB
  PrefetchGlobalRead: True
  DirectToLds: False
  MacroTileShapeMax: 64
  GlobalReadCoalesceVectorB: True
  GlobalReadCoalesceVectorA: True
  MacroTileShapeMin: 1
  GlobalLoadVectorWidthB: 2
  BufferStore: True
  NumLoadsA: 1
  NumLoadsB: 1
  BufferLoad: True
  SubGroupA: 16
  SubGroupB: 16
  ThreadTile0: 4
  NonTemporalA: 0
  AggressivePerfMode: 1
  NonTemporalB: 0
  ProblemType: Cijk_Ailk_Bjlk_DB
  ExpandPointerSwap: True
  ThreadTile: [4, 4]
  SubGroup0: 16
  ThreadTileB: 4
  ThreadTileA: 4
  PerformanceWaitLocation: -1
  AssignedDerivedParameters: True
  DisableKernelPieces: 0
  GlobalSplitU: 1
  InnerUnroll: 1
  CheckDimOverflow: 0
  LocalDotLayout: 1
  LdsOffsetB_Blk: 1536
  DirectToLdsA: False
  DirectToLdsB: False
  Valid: True
  LVPB: 4
  LSPB: 8
  EdgeType: ShiftPtr
  AssertFree1ElementMultiple: 1
  PerformanceSyncLocation: -1
  GlobalReadVectorWidth: 2
  WorkGroup: [16, 16, 1]
  LocalSplitU: 1
  WorkGroupMapping: 8
  LdsPadA: 0
  LdsPadB: 0
  GlobalReadCoalesceGroupB: True
  GlobalReadCoalesceGroupA: True
  NumLoadsCoalescedA: 1
  NumLoadsCoalescedB: 1
  GlobalSplitUSummationAssignmentRoundRobin: True
  NumGlobalWriteVectorsPerThread: 8
  LVCB: 32
  LVCA: 32
  CheckTensorDimAsserts: False
  LoopDoWhile: False
  LocalWrite2A: True
  LocalWrite2B: True
  MaxOccupancy: 40
  ThreadTile1: 4
  DepthU: 8
  UnrollMemFence: False
  LSPA: 8
  GlobalLoadVectorWidthA: 2
  GlobalWriteVectorWidth: 2
  LdsNumElements: 2048
  SubGroup1: 16
  GlobalSplitUWorkGroupMappingRoundRobin: False
  LSCA: 64
  PersistentKernel: 0
  LSCB: 64
  GuaranteeNoPartialA: False
  LocalWriteUseSgprA: False
  GuaranteeNoPartialB: False
  GlobalRead2B: True
  GlobalRead2A: True
  LoopUnroll: 8
  SuppresssNoLoadLoop: True
  LdsNumElementsAlignedA: 512
  LdsNumElementsAlignedB: 512
  MinGlobalWriteVectorWidth: 1
  LVPA: 4
  VectorAtomicWidth: 1
  KernelLanguage: Assembly
  MacroTileB: 64
  LocalWriteUseSgprB: False
  PerformanceWaitCount: -1
  MacroTileA: 64
  LocalRead2A: True
  LocalRead2B: True
  NonTemporalC: 0
  VectorWidth: 2
  FractionalLoad: 0
  UseSgprForGRO: False
  VectorStore: True
  LoopTail: True
  AssignedProblemIndependentDerivedParameters: True
  NumElementsPerThread: 16
  AssertFree0ElementMultiple: 1
  PrefetchLocalRead: True
  LdsOffsetA_Blk: 1024
  WorkGroupMappingType: B
  AssertSummationElementMultiple: 1
  MacroTile0: 64
  MacroTile1: 64
  NumThreads: 256
  NumLoadsPerpendicularA: 1
  LdsOffsetA: 0
  LdsOffsetB: 512
  NumLoadsPerpendicularB: 1
*/

